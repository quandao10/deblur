{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secret-designer",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torchvision\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-inventory",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "### Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-gamma",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "class Text(Dataset):\n",
    "    def __init__(self, path, img_transform = None, blur_transform = None):\n",
    "        self.path = path\n",
    "        self.no_images = len(os.listdir(self.path))//3\n",
    "        self.img_transform = img_transform\n",
    "        self.blur_transform = blur_transform\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.no_images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        assert 0<=idx<=self.no_images-1, \"Invalid index\"\n",
    "        \n",
    "        image_prefix = (7-len(str(idx)))*\"0\" + str(idx)\n",
    "        blur_img_path = os.path.join(self.path, image_prefix + \"_blur.png\")\n",
    "        blur_path = os.path.join(self.path, image_prefix + \"_psf.png\")\n",
    "        clean_img_path = os.path.join(self.path, image_prefix + \"_orig.png\")\n",
    "        \n",
    "        blur_kernel = Image.open(blur_path)\n",
    "        blur_img = Image.open(blur_img_path)\n",
    "        clean_img = Image.open(clean_img_path)\n",
    "        \n",
    "                \n",
    "        if self.img_transform:\n",
    "            blur_img = self.img_transform(blur_img)\n",
    "            clean_img = self.img_transform(clean_img)\n",
    "        if self.blur_transform:\n",
    "            blur_kernel = self.blur_transform(blur_kernel)\n",
    "        \n",
    "        return blur_kernel, blur_img, clean_img\n",
    "    \n",
    "\n",
    "img_transform = Compose([\n",
    "    Resize((256,256)),\n",
    "    ToTensor(),\n",
    "])\n",
    "\n",
    "blur_transform = Compose([\n",
    "    Resize((17,17)),\n",
    "    ToTensor(),\n",
    "])\n",
    "text = Text(\"/notebooks/dataset/text/data/\", img_transform, blur_transform)\n",
    "no_train = int(0.8*len(text))\n",
    "no_test = len(text)-no_train\n",
    "train_data, test_data = torch.utils.data.random_split(text, [no_train, no_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-cleaning",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-august",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mechanical-costa",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# taken from https://github.com/SeungjunNah/DeepDeblur-PyTorch/blob/master/src/model/common.py but change default_act to LeakyReLU\n",
    "\n",
    "def default_act():\n",
    "    return nn.LeakyReLU(True)\n",
    "\n",
    "def default_conv(in_channels, out_channels, kernel_size, bias=True, groups=1):\n",
    "    return nn.Conv2d(in_channels, out_channels, kernel_size, padding=(kernel_size // 2), bias=bias, groups=groups)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_feats, kernel_size, bias=True, conv=default_conv, norm=False, act=default_act):\n",
    "        super(ResBlock, self).__init__()\n",
    "        modules = []\n",
    "        for i in range(2):\n",
    "            modules.append(conv(n_feats, n_feats, kernel_size, bias=bias))\n",
    "            if norm: modules.append(norm(n_feats))\n",
    "            if act and i == 0: modules.append(act())\n",
    "        self.body = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res = self.body(x)\n",
    "        res += x\n",
    "        return res\n",
    "\n",
    "# layer = ResBlock(64, 3, norm = nn.BatchNorm2d).to(\"cuda\")\n",
    "# summary(layer, (64,8,8))\n",
    "\n",
    "# taken from https://github.com/yuanjunchai/IKC/blob/master/codes/models/modules/sftmd_arch.py\n",
    "    \n",
    "class SFT_Layer(nn.Module):\n",
    "    def __init__(self, nf=64, para=10):\n",
    "        super(SFT_Layer, self).__init__()\n",
    "        self.mul_conv1 = nn.Conv2d(para + nf, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.mul_leaky = nn.LeakyReLU(0.2)\n",
    "        self.mul_conv2 = nn.Conv2d(32, nf, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "        self.add_conv1 = nn.Conv2d(para + nf, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.add_leaky = nn.LeakyReLU(0.2)\n",
    "        self.add_conv2 = nn.Conv2d(32, nf, kernel_size=3, stride=1, padding=1)\n",
    "\n",
    "    def forward(self, feature_maps, para_maps):\n",
    "        cat_input = torch.cat((feature_maps, para_maps), dim=1)\n",
    "        mul = torch.sigmoid(self.mul_conv2(self.mul_leaky(self.mul_conv1(cat_input))))\n",
    "        add = self.add_conv2(self.add_leaky(self.add_conv1(cat_input)))\n",
    "        return feature_maps * mul + add\n",
    "\n",
    "\n",
    "# layer = SFT_Layer().to(\"cuda\")\n",
    "# summary(layer, [(64,8,8), (10, 8, 8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-venice",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# custom U-NET taken from https://amaarora.github.io/2020/09/13/unet.html#u-net\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_res_block=2, \n",
    "                 n_feats=64, \n",
    "                 kernel_size=3, \n",
    "                 norm=nn.BatchNorm2d, \n",
    "                 n_kernel=10):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(n_feats, kernel_size, norm = nn.BatchNorm2d) for i in range(n_res_block)])\n",
    "        self.sft_layer = SFT_Layer(n_feats, n_kernel)\n",
    "    \n",
    "    def forward(self, img, blur_kernel):\n",
    "        x = img\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = self.sft_layer(x, blur_kernel)\n",
    "        return x\n",
    "\n",
    "# encoder_block = EncoderBlock().to(\"cuda\")\n",
    "# summary(encoder_block, [(64,8,8), (10,8,8)])\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_scales=5, \n",
    "                 n_feats=64, \n",
    "                 n_kernel=10):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.enc_blocks = nn.ModuleList([EncoderBlock(n_feats = n_feats, n_kernel=n_kernel) for i in range(n_scales)])\n",
    "        self.downsample_layer = nn.Conv2d(n_feats, n_feats, kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample_kernel_layer = nn.Conv2d(n_kernel, n_kernel, kernel_size=3, stride=2, padding=1)\n",
    "    \n",
    "    def forward(self, img, blur_kernel):\n",
    "        x = img\n",
    "        y = blur_kernel\n",
    "        encoder_blocks = []\n",
    "        blur_blocks = []\n",
    "        for block in self.enc_blocks:\n",
    "            x = block(x, y)\n",
    "            encoder_blocks.append(x)\n",
    "            blur_blocks.append(y)\n",
    "            x = self.downsample_layer(x)\n",
    "            y = self.downsample_kernel_layer(y)\n",
    "        return encoder_blocks, blur_blocks\n",
    "\n",
    "# print(\"-------Test encoder--------\")\n",
    "# encoder = Encoder().to(\"cuda\")\n",
    "# enc_list, blur_list = encoder(torch.rand(1,64,64,64, device=\"cuda\"), torch.rand(1,10,64,64, device=\"cuda\"))\n",
    "# for enc in enc_list:\n",
    "#     print(enc.size())\n",
    "# print(\"---blur---\")\n",
    "# for blur in blur_list:\n",
    "#     print(blur.size())\n",
    "# print(\"---------------------------\")\n",
    "\n",
    "    \n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_res_block=2, \n",
    "                 n_feats=64, \n",
    "                 kernel_size=3, \n",
    "                 norm=nn.BatchNorm2d, \n",
    "                 n_kernel=10):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.reduce_channel_conv = nn.Conv2d(n_feats*2, n_feats, kernel_size=1)\n",
    "        self.res_blocks = nn.ModuleList([ResBlock(n_feats, kernel_size, norm = nn.BatchNorm2d) for i in range(n_res_block)])\n",
    "        self.sft_layer = SFT_Layer(n_feats, n_kernel)\n",
    "    \n",
    "    def forward(self, img, blur_kernel):\n",
    "        x = self.reduce_channel_conv(img)\n",
    "        for block in self.res_blocks:\n",
    "            x = block(x)\n",
    "        x = self.sft_layer(x, blur_kernel)\n",
    "        return x\n",
    "    \n",
    "# decoder_block = DecoderBlock().to(\"cuda\")\n",
    "# summary(decoder_block, [(128,8,8), (10,8,8)])\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 n_scales=4, \n",
    "                 n_feats=64):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.upsample_layer = nn.ConvTranspose2d(n_feats, n_feats, kernel_size=3, stride=2)\n",
    "        self.dec_blocks = nn.ModuleList([DecoderBlock(n_feats=n_feats) for i in range(n_scales)]) \n",
    "        \n",
    "    def forward(self, encoder_blocks, blur_blocks):\n",
    "        x = encoder_blocks.pop()\n",
    "        y = blur_blocks.pop()\n",
    "        for block in self.dec_blocks:\n",
    "            x = self.upsample_layer(x)\n",
    "            z = encoder_blocks.pop()\n",
    "            y = blur_blocks.pop()\n",
    "            x = self.crop(x, z)\n",
    "            x = torch.cat([x, z], dim=1)\n",
    "            x = block(x, y)\n",
    "        return x\n",
    "    \n",
    "    def crop(self, enc_ftrs, x):\n",
    "        _, _, H, W = x.shape\n",
    "        enc_ftrs   = torchvision.transforms.CenterCrop([H, W])(enc_ftrs)\n",
    "        return enc_ftrs\n",
    "\n",
    "# print(\"-------Test decoder--------\")\n",
    "# decoder = Decoder().to(\"cuda\")\n",
    "# dec = decoder(enc_list, blur_list)\n",
    "# print(dec.size())\n",
    "# print(\"---------------------------\")\n",
    "\n",
    "\n",
    "class DeblurNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_img_channel=3,\n",
    "                 n_scales=4, \n",
    "                 n_feats=64, \n",
    "                 n_kernel=10):\n",
    "        super(DeblurNet, self).__init__()\n",
    "        self.encoder = Encoder(n_scales=n_scales+1, n_feats=n_feats, n_kernel=n_kernel)\n",
    "        self.decoder = Decoder(n_scales=n_scales, n_feats=n_feats)\n",
    "        self.conv_in = nn.Conv2d(n_img_channel, n_feats, kernel_size=7, padding=3)\n",
    "        self.conv_mean_out = nn.Conv2d(n_feats, n_img_channel, kernel_size=7, padding=3)\n",
    "        self.conv_var_out = nn.Conv2d(n_feats, n_img_channel, kernel_size=7, padding=3)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.batchnorm = nn.BatchNorm2d(n_img_channel)\n",
    "\n",
    "    def forward(self, img, blur_kernel):\n",
    "        x = self.conv_in(img)\n",
    "        enc_list, blur_list = self.encoder(x, blur_kernel)\n",
    "        x = self.decoder(enc_list, blur_list)\n",
    "        mean = self.conv_mean_out(x)\n",
    "        mean = self.sigmoid(mean)\n",
    "        var = self.conv_var_out(x)\n",
    "        var = self.sigmoid(var)\n",
    "        return mean, var\n",
    "\n",
    "# print(\"------Test DeblurNet-------\")\n",
    "# deblur = DeblurNet().to(\"cuda\")\n",
    "# mu, var = deblur(torch.rand(2,3,256,256,device=\"cuda\"), torch.rand(2,10,256,256,device=\"cuda\"))\n",
    "# print(mu.size())\n",
    "# print(var.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-trauma",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "# KernelNet network \n",
    "\n",
    "class KernelNet(nn.Module):\n",
    "    def __init__(self, blur_kernek_size, pretrain_resnet=\"resnet18\"):\n",
    "        super(KernelNet, self).__init__()\n",
    "        self.blur_kernek_size = blur_kernek_size\n",
    "        if pretrain_resnet == \"resnet18\":\n",
    "            self.pretrain = models.resnet18(pretrained=True)\n",
    "        elif pretrain_resnet == \"resnet34\":\n",
    "            self.pretrain = models.resnet34(pretrained=False)\n",
    "        for param in self.pretrain.parameters():\n",
    "            param.requires_grad = False\n",
    "        num_ftrs = self.pretrain.fc.out_features\n",
    "        self.linear = nn.Linear(num_ftrs, self.blur_kernek_size**2)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        res = self.pretrain(x)\n",
    "        res = self.linear(res)\n",
    "        res = self.softmax(res)\n",
    "        return res.view(-1,  self.blur_kernek_size, self.blur_kernek_size)\n",
    "\n",
    "# print(\"------Test KernelNet-------\")\n",
    "# kernel_net_model = KernelNet(25).to(\"cuda\")\n",
    "# result = kernel_net_model(torch.rand(4,3,64,64,device=\"cuda\"))\n",
    "# print(result.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-priest",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "class VB_Blur(nn.Module):\n",
    "    def __init__(self, kernel_size = 17, n_scales=4, n_feats=64, n_kernel=10, img_size = 256):\n",
    "        super(VB_Blur, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.kernel_size = kernel_size\n",
    "        self.deblur_net = DeblurNet(n_img_channel=3, n_scales=n_scales, n_feats=n_feats, n_kernel=n_kernel)\n",
    "        self.kernel_net = KernelNet(kernel_size)\n",
    "        self.t_conv_layer = nn.ConvTranspose2d(1, n_kernel, kernel_size=3, stride=3)\n",
    "        \n",
    "    def forward(self, blur_img):\n",
    "        batch_size, _, _, _ = blur_img.size()\n",
    "        simplex = self.kernel_net(blur_img)\n",
    "        simplex = simplex+1e-20\n",
    "        blur_dist = torch.distributions.dirichlet.Dirichlet(simplex.view(batch_size, -1))\n",
    "        blur_kernel = blur_dist.sample().view(batch_size, self.kernel_size, self.kernel_size)\n",
    "        \n",
    "        blur_kernel_norm = self.t_conv_layer(blur_kernel.view(-1, 1, self.kernel_size, self.kernel_size))\n",
    "        blur_kernel_norm = Resize(self.img_size)(blur_kernel_norm)\n",
    "        \n",
    "        mu, log_var = self.deblur_net(blur_img, blur_kernel_norm)\n",
    "#         print(mu)\n",
    "        \n",
    "        img_dist = torch.distributions.normal.Normal(mu*255, torch.exp(log_var))\n",
    "        latent_img = img_dist.sample()\n",
    "        latent_img = torch.clip(latent_img, 0, 255).to(torch.uint8)/255\n",
    "        \n",
    "        return mu, log_var, simplex, blur_kernel, latent_img\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "vb_deblur = VB_Blur().to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-export",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abroad-military",
   "metadata": {
    "gradient": {
     "editing": false
    }
   },
   "outputs": [],
   "source": [
    "def gaussian_kl_loss(mu1, log_var1, mu2, epsilon= 1e-2):\n",
    "    batch_size, channel, image_size, _ = mu1.size()\n",
    "    var_ratio = torch.exp(log_var1)**2/epsilon**2\n",
    "    temp = 0.5*(torch.pow(mu1 - mu2, 2)/epsilon**2 + var_ratio - 2*torch.log2(var_ratio) - 1)\n",
    "    return temp.sum()/(batch_size)\n",
    "\n",
    "# print(gaussian_kl_loss(mu, log_var, latent_img))\n",
    "    \n",
    "def diriclet_kl_loss(simplex, blur_kernel, scaling_term = 2e4):\n",
    "    batch_size, kernel_size, _ = simplex.size()\n",
    "    blur_kernel = blur_kernel.squeeze()\n",
    "    blur_kernel += 1e-20\n",
    "    blur_kernel = blur_kernel*scaling_term\n",
    "    \n",
    "    sum_simplex = torch.sum(simplex, axis=[1,2])\n",
    "    sum_blur = torch.sum(blur_kernel, axis=[1,2])\n",
    "    \n",
    "    lgamma_sum_simplex = torch.lgamma(sum_simplex).sum()\n",
    "    lgamma_sum_blur = torch.lgamma(sum_blur).sum()\n",
    "    lgamma_simplex = torch.lgamma(simplex).sum()\n",
    "    lgamma_blur = torch.lgamma(blur_kernel).sum()\n",
    "    \n",
    "    diff_digamma = torch.digamma(simplex) - torch.digamma(sum_simplex).view(-1, 1, 1)\n",
    "    diff_term = simplex - blur_kernel\n",
    "    diff = (diff_term*diff_digamma).sum()\n",
    "            \n",
    "    return (lgamma_sum_simplex - lgamma_sum_blur - lgamma_simplex + lgamma_blur + diff)/(batch_size)\n",
    "\n",
    "# print(diriclet_kl_loss(simplex, blur_kernel))\n",
    "\n",
    "\n",
    "def filter_gauss(latent_img, blur_kernel):\n",
    "    c, h, w = latent_img.size()\n",
    "    k, k = blur_kernel.size()\n",
    "    blur_kernel = blur_kernel.view(1, 1, k, k)\n",
    "    channels = []\n",
    "    for i in range(c):\n",
    "        blur_channel = latent_img[i].view(1,1,h,w)\n",
    "        blur_result = torch.nn.functional.conv2d(blur_channel, blur_kernel, stride=[1,1])\n",
    "        channels.append(blur_result)\n",
    "    return torch.vstack(channels).squeeze()\n",
    "\n",
    "\n",
    "def gaussian_loss(blur_img ,blur_kernel_gen, latent_img_gen, image_size=256, sigma = 1e-2):\n",
    "    loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    batch_size, channel, image_size, _ = latent_img_gen.size()\n",
    "    constant = (image_size**2)*torch.log(sigma/torch.sqrt(2*torch.tensor(np.pi, device=\"cuda\")))\n",
    "    for i in range(batch_size):\n",
    "        blur_img_gen = filter_gauss(latent_img_gen[i], blur_kernel_gen[i])\n",
    "        blur_img_gen = Resize(image_size)(blur_img_gen)\n",
    "        loss = loss + constant - 0.5*((blur_img - blur_img_gen)**2/sigma**2).sum()\n",
    "    return loss/(batch_size)\n",
    "        \n",
    "# print(gaussian_loss(blur_img ,blur_kernel_gen, latent_img_gen, image_size=256, sigma = 1e-2))\n",
    "\n",
    "def loss_vb(blur_kernel, \n",
    "            blur_img, \n",
    "            latent_img, \n",
    "            mu, \n",
    "            log_var, \n",
    "            simplex, \n",
    "            blur_kernel_gen, \n",
    "            latent_img_gen, \n",
    "            image_size=256, \n",
    "            sigma = 1e-2, \n",
    "            scaling_term = 2e4):\n",
    "    kl_gaussian = gaussian_kl_loss(mu, log_var, latent_img)\n",
    "    kl_diriclet = diriclet_kl_loss(simplex, blur_kernel, scaling_term=scaling_term)\n",
    "    gauss_loss = gaussian_loss(blur_img ,blur_kernel_gen, latent_img_gen, image_size=image_size, sigma = sigma)\n",
    "    total_loss = kl_gaussian + kl_diriclet - gauss_loss\n",
    "    return total_loss/1e9\n",
    "    \n",
    "# print(loss_vb(blur_kernel, blur_img, latent_img, mu, log_var, simplex, blur_kernel_gen, latent_img_gen))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-bubble",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-placement",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(vb_deblur.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-apparel",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, run, image_size=256, sigma = 1e-2, scaling_term = 2e4):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    total_loss = torch.tensor(0.0, device=\"cuda\")\n",
    "    for batch, (blur_kernel, blur_img, latent_img) in enumerate(dataloader):\n",
    "        blur_kernel, blur_img, latent_img = blur_kernel.to(\"cuda\"), blur_img.to(\"cuda\"), latent_img.to(\"cuda\")\n",
    "        mu, log_var, simplex, blur_kernel_gen, latent_img_gen = model(blur_img)\n",
    "        loss = loss_vb(blur_kernel, \n",
    "                       blur_img, \n",
    "                       latent_img, \n",
    "                       mu, \n",
    "                       log_var, \n",
    "                       simplex, \n",
    "                       blur_kernel_gen, \n",
    "                       latent_img_gen, \n",
    "                       image_size=image_size, \n",
    "                       sigma = sigma, \n",
    "                       scaling_term = scaling_term)\n",
    "        total_loss += loss\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch%100==0:\n",
    "            loss, current = loss.item(), batch * len(blur_kernel)\n",
    "            print(\"loss: {}  [{}/{}]\".format(loss, current, size))\n",
    "    ave_loss = total_loss/len(dataloader)\n",
    "    run[\"train/loss\"].log(ave_loss)\n",
    "    print(\"average loss: {}\".format(ave_loss))\n",
    "    torch.save({'model_state_dict': model.state_dict(),'optimizer_state_dict': optimizer.state_dict()}, 'checkpoint2.pth')\n",
    "    return ave_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satisfactory-indonesian",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "def validation():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-authentication",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# !pip install neptune_notebooks\n",
    "import neptune.new as neptune\n",
    "\n",
    "run = neptune.init(\n",
    "    project=\"kevinqd/image-deblur\",\n",
    "    api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJhZDYzNWE1NS05NjliLTQ5YjQtYmRhNS0xNTE2NzNlN2E2NjEifQ==\",\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reserved-allergy",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "hists = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    hist = train(vb_deblur, test_dataloader, optimizer, run, image_size=256, sigma = 1e-2, scaling_term = 2e4)\n",
    "    hists.append(hist)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-cancellation",
   "metadata": {},
   "source": [
    "### test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-louisville",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# vb_deblur.load_state_dict(torch.load(\"/notebooks/checkpoint.pth\")['model_state_dict'])\n",
    "# vb_deblur.eval()\n",
    "\n",
    "# blur_kernel, blur_img, clean_img = train_data[0]\n",
    "# mu, log_var, simplex, blur_kernel, latent_img = vb_deblur(blur_img.view(1, 3, 256, 256).to(\"cuda\"))\n",
    "\n",
    "# plt.imshow(np.transpose(clean_img.cpu().squeeze(), (1,2,0)))\n",
    "# plt.savefig(\"clean.png\")\n",
    "\n",
    "# plt.imshow(np.transpose(blur_img.cpu().squeeze(), (1,2,0)))\n",
    "# plt.savefig(\"blur.png\")\n",
    "\n",
    "# plt.imshow(np.transpose(latent_img.cpu().squeeze(), (1,2,0)))\n",
    "# plt.savefig(\"gen.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-habitat",
   "metadata": {
    "gradient": {}
   },
   "source": [
    "### some function to test theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-brain",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "# img = train_data[0][-1]*255\n",
    "\n",
    "# blur = train_data[0][1]\n",
    "\n",
    "# var = torch.ones_like(img)*2\n",
    "\n",
    "\n",
    "# img_dist = torch.distributions.normal.Normal(img, var)\n",
    "# latent_img = img_dist.sample() \n",
    "\n",
    "\n",
    "\n",
    "# blur_gauss = torchvision.transforms.GaussianBlur(15, sigma=(1.5, 1.5))\n",
    "# latent_img2 = blur_gauss(img)\n",
    "\n",
    "# print(blur_gauss)\n",
    "\n",
    "# # img_dist = torch.distributions.multivariate_normal.MultivariateNormal(img.view(-1, 3), var.view(-1,3))\n",
    "# # multi_latent_img = img_dist.rsample().view(3, 256, 256)\n",
    "\n",
    "# plt.plot()\n",
    "# plt.imshow(np.transpose(np.clip(img.to(torch.uint8), 0, 255), (1,2, 0)))\n",
    "\n",
    "# plt.plot()\n",
    "# plt.imshow(np.transpose(np.clip(blur, 0, 1), (1,2, 0)))\n",
    "\n",
    "# plt.plot()\n",
    "# plt.imshow(np.transpose(np.clip(latent_img, 0, 255).to(torch.uint8), (1,2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-dance",
   "metadata": {
    "gradient": {}
   },
   "outputs": [],
   "source": [
    "# kernel = train_data[0][0]\n",
    "# kernel = kernel+1e-5\n",
    "\n",
    "# blur_dist = torch.distributions.dirichlet.Dirichlet(kernel.view(1, -1))\n",
    "# blur_kernel = blur_dist.sample().view(17,17)\n",
    "# plt.plot()\n",
    "# plt.imshow(blur_kernel)\n",
    "\n",
    "# def filterGauss(latent_img, blur_kernel):\n",
    "#     c, h, w = latent_img.size()\n",
    "#     k, k = blur_kernel.size()\n",
    "#     blur_kernel = blur_kernel.view(1, 1, k, k)\n",
    "#     channels = []\n",
    "#     for i in range(c):\n",
    "#         blur_channel = latent_img[i].view(1,1,h,w)\n",
    "#         blur_result = torch.nn.functional.conv2d(blur_channel, blur_kernel, stride=[1,1])\n",
    "#         channels.append(blur_result)\n",
    "#     return torch.vstack(channels).squeeze()\n",
    "\n",
    "# blur_result = filterGauss(latent_img, blur_kernel)\n",
    "\n",
    "# plt.imshow(np.transpose(np.clip(blur_result, 0, 255).to(torch.uint8), (1,2,0)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
